## Pretrained model

### XLnet

初步实验和nni调参，调参中尝试了较大的模型，模型的hidden层较大（768，1024），发现模型较大时必须要使用很小的学习率训练才能收敛（1e-4, 1e-5），但调参过程实际意义不大，最终使用的还是接近于论文中的参数设置，详细设置参考代码。

XLnet模型在实验时仅用作词向量生成器，不参与模型训练，即没有微调。实验时观察到了验证集上的过拟合现象，也尝试过保存最小的验证集上loss的模型（早停）而不是最好的F1score的模型，但在test上结果表现差别不大。

### Roberta

Roberta（1.3G）相对于XLnet（800M）更大，换用roberta后没有再对参数进行调整，依然出现了验证集上的过拟合现象，但F1分数会持续上升并最终小范围波动。最后分别取验证集loss最小的模型和F1score最高模型，后者表现更好。

##### 微调模型

最终由于时间和空间的问题，只尝试了对Roberta模型进行了微调，微调roberta时模型需要优化的参数量显著上升，模型loss下降速度和F1score上升速度均有较大幅度提升，微调Roberta并训练时没有观察到验证集上的过拟合现象，验证集上的loss一直在小范围内波动，最终保存了F1score分数最高的模型用于解码。遗憾的时没有赶上A榜截至提交时间，提交了B榜，等待B榜公布结果中。。。