# loss计算优化(mask)

当前的loss计算可能存在较大稀疏性的问题，对于label中大多数位置均为零，于是模型就会学习到将所有位置的span都预测位无类别，可从loss计算的角度进行一些优化。

当前基本思路是将loss中与预测结果无关的下三角部分mask掉，以及将loss矩阵中的padding也mask掉。

计算loss时输入的batch矩阵为$(b, l, l, c),(b,l,l)$ ，其中$l$ 为padding后batch中句子的最大长度，首先需要将较短句子的padding部分mask掉，可以利用随着数据加载过程得到的mask矩阵实现；然后是解码无用的下三角矩阵，对于一个$(l,l)$ 的预测类别矩阵，我们所关注的是其中$(i,j)$ 位置的元素，代表着起止分别为i和j的一个span的预测概率，且要求$i\leq j$ ，所以$(l,l)$ 的预测矩阵只有上三角的元素在解码时是有效的，计算loss时可以直接将下三角的预测结果全部mask掉，节省loss计算的计算量和模型优化的压力。

有两种实现思路：

- 将计算无关的pred向量mask并将label也mask，使其计算的loss为零，即不会传播梯度信息
- 对输出矩阵和label矩阵做提取操作，只提取出相关的预测向量和label来计算loss，已经过验证index_select后的矩阵可根据位置正常传递梯度。优先选择该方法。（实际采用的方法）